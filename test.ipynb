{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ca8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_config_path TRAIN_CONFIG_PATH] [--model_config_path MODEL_CONFIG_PATH]\n",
      "                             [--data_config_path DATA_CONFIG_PATH] [--model_name MODEL_NAME]\n",
      "                             [--model_save_path MODEL_SAVE_PATH] [--result_save_dir_path RESULT_SAVE_DIR_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\86133\\AppData\\Roaming\\jupyter\\runtime\\kernel-c86df39b-fd6d-499b-bc6f-1c19a3f2dc83.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86133\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainers\n",
    "import models\n",
    "import datasets\n",
    "import data_preprocessors\n",
    "\n",
    "from utils import scaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def load_config(data_path):\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    result_save_dir_path = args.result_save_dir_path\n",
    "    model_save_path = args.model_save_path\n",
    "    if not os.path.exists(result_save_dir_path):\n",
    "        os.makedirs(result_save_dir_path)\n",
    "    if not os.path.exists(os.path.dirname(model_save_path)):\n",
    "        os.makedirs(os.path.dirname(model_save_path))\n",
    "   \n",
    "    train_config = load_config(args.train_config_path)\n",
    "    model_config = load_config(args.model_config_path)\n",
    "    data_config = load_config(args.data_config_path)\n",
    "\n",
    "    # basic config\n",
    "    random_seed = train_config[\"random_seed\"]\n",
    "    device = torch.device(train_config[\"device\"])\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    load_checkpoint = train_config[\"load_checkpoint\"]\n",
    "\n",
    "    # set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # ----------------------- Load data ------------------------\n",
    "    # dataset\n",
    "    # datasets_class = getattr(sys.modules[\"datasets\"], data_config[\"dataset_name\"])\n",
    "    # datasets = datasets_class(**data_config[\"dataset_params\"])\n",
    "    \n",
    "    train_data,valid_data,test_data,train_dataloader, test_dataloader, valid_dataloader = getattr(sys.modules[\"datasets\"], data_config[\"dataset_name\"]).get_data_loaders(\n",
    "        data_path=data_config['dataset_params']['data_path'],\n",
    "        sequence_len=data_config['dataset_params']['sequence_len'],\n",
    "        sub_dataset=data_config['dataset_params']['sub_dataset'],\n",
    "        norm_type=data_config['dataset_params']['norm_type'],\n",
    "        max_rul=data_config['dataset_params']['max_rul'],\n",
    "        cluster_operations=data_config['dataset_params']['cluster_operations'],\n",
    "        norm_by_operations=data_config['dataset_params']['norm_by_operations'],\n",
    "        use_max_rul_on_test=data_config['dataset_params']['use_max_rul_on_test'],\n",
    "        validation_rate=data_config['dataset_params']['validation_rate'],\n",
    "        return_id=True,\n",
    "        use_only_final_on_test=data_config['dataset_params']['use_only_final_on_test'],\n",
    "        loader_kwargs={'batch_size':data_config['dataset_params']['batch_size']}\n",
    "    )\n",
    "\n",
    "    # train_data, valid_data, test_data = data_preprocessor.split_data(preprocessed_data)\n",
    "    \n",
    "    # train_dataset = dataset_class(train_data, type=\"train\", **data_config[\"dataset_params\"])\n",
    "    # valid_dataset = dataset_class(valid_data, type=\"valid\", **data_config[\"dataset_params\"])\n",
    "    # test_dataset = dataset_class(test_data, type=\"test\", **data_config[\"dataset_params\"])\n",
    "    # train_dataloader = DataLoader(\n",
    "    #     train_dataset, batch_size=batch_size, **data_config[\"dataloader_params\"]\n",
    "    # )\n",
    "    # valid_dataloader = DataLoader(\n",
    "    #     valid_dataset, batch_size=batch_size, **data_config[\"dataloader_params\"]\n",
    "    # )\n",
    "    # test_dataloader = DataLoader(\n",
    "    #     test_dataset, batch_size=batch_size, **data_config[\"dataloader_params\"]\n",
    "    # )\n",
    "\n",
    "\n",
    "    #data_preprocessor_class = getattr(\n",
    "        #sys.modules[\"data_preprocessors\"], data_config[\"data_preprocessor_name\"]\n",
    "    #)\n",
    "    #dcata_preproessor = data_preprocessor_class(**data_config[\"data_preprocessor_params\"])\n",
    "    #preprocessed_data = data_preprocessor.preprocess()\n",
    "\n",
    "    \n",
    "\n",
    "    # update model & trainer params\n",
    "    # data_config[\"dataset_params\"].update(data_preprocessor.update_dataset_params)\n",
    "    # model_config[args.model_name].update(data_preprocessor.update_model_params)\n",
    "    # train_config[\"trainer_params\"].update(data_preprocessor.update_trainer_params)\n",
    "\n",
    "    # scale data\n",
    "    # scaler_class = getattr(sys.modules[\"utils.scaler\"], data_config[\"scaler_name\"])\n",
    "    # scaler = scaler_class(**data_config[\"scaler_params\"])\n",
    "    # scaler.fit(train_data)\n",
    "    # train_data = scaler.transform(train_data)\n",
    "    # valid_data = scaler.transform(valid_data)\n",
    "    # test_data = scaler.transform(test_data)\n",
    "\n",
    "    \n",
    "    # ------------------------- Model ---------------------------\n",
    "\n",
    "    # model\n",
    "    model_class = getattr(sys.modules[\"models\"], args.model_name)\n",
    "    model = model_class(**model_config[args.model_name])\n",
    "    model.to(device)\n",
    "    # ------------------------- Trainer -------------------------\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_class = getattr(\n",
    "        sys.modules[\"torch.optim\"], train_config[\"optimizer_name\"]\n",
    "    )\n",
    "    optimizer = optimizer_class(model.parameters(), **train_config[\"optimizer_params\"])\n",
    "\n",
    "    # scheduler\n",
    "    scheduler_class = getattr(\n",
    "        sys.modules[\"torch.optim.lr_scheduler\"], train_config[\"scheduler_name\"]\n",
    "    )\n",
    "    scheduler = scheduler_class(optimizer, **train_config[\"scheduler_params\"])\n",
    "\n",
    "    print(model_save_path)\n",
    "    # trainer\n",
    "    trainer_class = getattr(sys.modules[\"trainers\"], train_config[\"trainer_name\"])\n",
    "\n",
    "    trainer = trainer_class(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        scaler,\n",
    "        model_save_path,\n",
    "        result_save_dir_path,\n",
    "        **train_config[\"trainer_params\"]\n",
    "    )\n",
    "\n",
    "    # load checkpoint\n",
    "    if load_checkpoint:\n",
    "        trainer.load_checkpoint()\n",
    "\n",
    "    # ------------------------- Train & Test ------------------------\n",
    "    config = {\n",
    "        \"args\": vars(args),\n",
    "        \"train_config\": train_config,\n",
    "        \"model_config\": model_config,\n",
    "        \"data_config.yaml\": data_config,\n",
    "    }\n",
    "    print(\"Configuration: \", config)\n",
    "    with open(os.path.join(result_save_dir_path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(\"Start training.\")\n",
    "\n",
    "    epoch_results = trainer.train(train_dataloader, valid_dataloader)\n",
    "    test_result, y_pred, y_true = trainer.test(test_dataloader)\n",
    "\n",
    "    # save y_pred, y_true to self.result_save_dir/y_pred.npy, y_true.npy\n",
    "    np.save(os.path.join(result_save_dir_path, \"test_y_pred.npy\"), y_pred)\n",
    "    np.save(os.path.join(result_save_dir_path, \"test_y_true.npy\"), y_true)\n",
    "\n",
    "    # save results\n",
    "    result = {\n",
    "        \"config\": config,\n",
    "        \"test_result\": test_result,\n",
    "        \"epoch_results\": epoch_results,\n",
    "    }\n",
    "    with open(os.path.join(result_save_dir_path, \"result.json\"), \"w\") as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_config_path\",\n",
    "        type=str,\n",
    "        default=\"./config/train_config/MHA_train_config.yaml\",\n",
    "        help=\"Config path of Trainer\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_config_path\",\n",
    "        type=str,\n",
    "        default=\"./config/model_config/MHA_model_config.yaml\",\n",
    "        help=\"Config path of models\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_config_path\",\n",
    "        type=str,\n",
    "        default=\"./config/data_config/CMAPSS_config.yaml\",\n",
    "        help=\"Config path of Data\",\n",
    "    )\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"MultiHeadAttentionLSTM\", help=\"Model name\")\n",
    "    parser.add_argument(\n",
    "        \"--model_save_path\",\n",
    "        type=str,\n",
    "        default=\"./model_states/MHA.pkl\",\n",
    "        help=\"Model save path\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--result_save_dir_path\",\n",
    "        type=str,\n",
    "        default=\"./results/MHA\",\n",
    "        help=\"Result save path\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef680f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
